## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

!pip install cmake
!pip install transformers
!pip install --upgrade pip
!pip install kiwipiepy
!pip install kiwipiepy transformers torch sentencepiece
!pip install scikit-learn

!pip install gradio

## ë°ì´í„° ì „ì²˜ë¦¬

import pandas as pd

# 1. ì—‘ì…€ íŒŒì¼ ì½ì–´ì˜¤ê¸°
excel_path1 = '/content/wellness_final.xlsx'  #ë™ì§„
excel_path2 = '/content/7label.xlsx'

df1 = pd.read_excel(excel_path1)
df2 = pd.read_excel(excel_path2)

df1_selected = df1.iloc[1:19770, [0, 1]] #ë™ì§„
df1_selected.columns = ['keyword', 'text']

df2_selected = df2.iloc[1:, [1, 0]] #í˜œë€
df2_selected.columns = ['keyword', 'text']
df2_selected = df2_selected[df2_selected['keyword'].isin(['í–‰ë³µ', 'ì¤‘ë¦½'])]

# í–‰ ë³‘í•©
df_merged = pd.concat([df1_selected, df2_selected], axis=0)

# ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡
exclude_keywords = ['ì¶©ë™', 'ë‚™ ì—†ìŒ', 'ì‹¤ë§', 'ëª¨ìš•ê°', 'ì ˆë°•', 'ê³¤ë€']
# ì œì™¸í•  í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” í–‰ ì œê±°
df_merged = df_merged[~df_merged['keyword'].isin(exclude_keywords)]

# NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
df_merged = df_merged.dropna()
# í‚¤ì›Œë“œ ê°’ ì¶œë ¥
print("Keywords in df_merged:")
print(df_merged['keyword'].unique())

# í…ìŠ¤íŠ¸ ì—´ ì§€ì •
texts = df_merged['text'].tolist()

# ì‹ ì¡°ì–´ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
df4 = pd.read_csv('./korean_slang.csv')
new_words_dict = pd.Series(df4.ëœ»í’€ì´.values, index=df4.ì‹ ì¡°ì–´).to_dict()

### ê°ì •ë¶„ì„ìš© ì „ì²˜ë¦¬

df3 = pd.read_excel(excel_path2, usecols=[1, 0], names=['text', 'label'])

# ë ˆì´ë¸”ì„ ìˆ«ìë¡œ ë§¤í•‘!pip install scikit-learn
label_mapping = {
    'í–‰ë³µ': 0,
    'ì¤‘ë¦½': 1,
    'ìŠ¬í””': 2,
    'ê³µí¬': 3,
    'í˜ì˜¤': 4,
    'ë¶„ë…¸': 5,
    'ë†€ëŒ': 6
}

# 'label' ì—´ì„ ìˆ«ìë¡œ ë§¤í•‘
df3['label'] = df3['label'].map(label_mapping)

# NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
df3 = df3.dropna(subset=['text'])
df3 = df3.dropna(subset=['label'])

df3['label'] = df3['label'].astype(int)

# í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
train_texts = df3['text'][1:].tolist()
train_texts = [str(text) for text in train_texts]
train_labels = df3['label'][1:].tolist()

# í‚¤ì›Œë“œ ë³‘í•© ê·œì¹™ ì ìš©
df_merged['keyword'] = df_merged['keyword'].replace({
    'ìˆ˜ë©´ì¥ì• ': 'ë¶ˆë©´', 'ì ì´ì•ˆì˜´': 'ë¶ˆë©´', 'ì ë“œëŠ”ë° ì–´ë ¤ì›€': 'ë¶ˆë©´',	'ìë‹¤ê¹¸': 'ë¶ˆë©´',	'ìë„í”¼ë¡œì•ˆí’€ë¦¼': 'ë¶ˆë©´',	'ìˆ˜ë©´ì‹œê°„ê°ì†Œ': 'ë¶ˆë©´',	'ì ì„¤ì¹¨': 'ë¶ˆë©´',	'ì–•ê²Œì ': 'ë¶ˆë©´',	'ëª½ë¡±': 'ë¶ˆë©´',	'ì¡°ê°ì ': 'ë¶ˆë©´',	'ë¹„ëª½ì‚¬ëª½': 'ë¶ˆë©´',	'í”¼ê³¤': 'ë¶ˆë©´',
    'ëˆˆë¬¼': 'ìŠ¬í””', 'ìš¸ìŒ': 'ìŠ¬í””',
    'ì‹ìš•ë¶€ì§„': 'ì‹ìš•ê°ì†Œ',	'ì²´ì¤‘ê°ì†Œ': 'ì‹ìš•ê°ì†Œ',	'ì…ë§›ê°ì†Œ': 'ì‹ìš•ê°ì†Œ',	'ì‚´ë¹ ì§': 'ì‹ìš•ê°ì†Œ',	'ì‹ì‚¬ëŸ‰ê°ì†Œ': 'ì‹ìš•ê°ì†Œ',
    'ìœ„ì¶•':'ìì¡´ê°ê°ì†Œ',	'ìì‹ ê°ê°ì†Œ':'ìì¡´ê°ê°ì†Œ',	'ì†Œì‹¬':'ìì¡´ê°ê°ì†Œ',	'í•˜ì°®ìŒ':'ìì¡´ê°ê°ì†Œ',	'ëª»ë‚¨':'ìì¡´ê°ê°ì†Œ',	'í•œì‹¬':'ìì¡´ê°ê°ì†Œ',	'ê°€ì¹˜ ì—†ìŒ':'ìì¡´ê°ê°ì†Œ',	'ë‚˜ì•½':'ìì¡´ê°ê°ì†Œ',
    'íŒ¨ë°°ì':'íŒ¨ë°°ê°',	'í¬ê¸°':'íŒ¨ë°°ê°',
    'ë¹„ì›ƒìŒ':'í”¼í•´ì˜ì‹',	'ì—…ì‹ ì—¬ê¹€':'í”¼í•´ì˜ì‹',	'ëˆˆì´ˆë¦¬':'í”¼í•´ì˜ì‹',	'ëƒ‰ì†Œ':'í”¼í•´ì˜ì‹',	'ê°ì‹œ':'í”¼í•´ì˜ì‹',	'í•´ì½”ì§€':'í”¼í•´ì˜ì‹',	'ì†ê°€ë½ì§ˆ':'í”¼í•´ì˜ì‹',	'ì§¸ë ¤ë³´ë‹¤':'í”¼í•´ì˜ì‹',
    'ì£„ì¸':'ì£„ì±…ê°',	'ì˜ëª»':'ì£„ì±…ê°',	'ì§ˆì±…':'ì£„ì±…ê°',	'ìì±…':'ì£„ì±…ê°',
    'ì•ˆíƒ€ê¹Œì›€':'ì—°ë¯¼',
    'í—ˆë¬´í•¨': 'í—ˆë¬´', 'í—ˆë§': 'í—ˆë¬´',	'ê³µí—ˆ': 'í—ˆë¬´',	'í—ˆíƒˆ': 'í—ˆë¬´', 'í—ˆë§í•¨':'í—ˆë¬´','í—ˆë¬´  ':'í—ˆë¬´',
    'í¬ë§ì—†ìŒ':'ì ˆë§ê°',	'ë‚­ë– ëŸ¬ì§€ ëì— ìˆëŠ” ëŠë‚Œ':'ì ˆë§ê°',
    'í›„íšŒí•¨': 'í›„íšŒ',
    'ì¢…ì¡ì„ìˆ˜ì—†ìŒ':'í˜¼ë€',	'ìš°ìœ ë¶€ë‹¨':'í˜¼ë€',	'ê²°ë‹¨ë ¥ì—†ìŒ':'í˜¼ë€',
    'ì“¸ì“¸í•¨': 'ì™¸ë¡œì›€', 'ì ì í•¨': 'ì™¸ë¡œì›€',	'ê²‰ëŒë‹¤': 'ì™¸ë¡œì›€',
    'ë¬´ì˜ë¯¸':'ë¬´ì˜ë¯¸í•¨',
    'ê³ ë…ê°':'ì™¸í†¨ì´',
    'ìš±í•¨':'í™”',
    'ê°ì •ê¸°ë³µ':'ì¡°ìš¸',
    'ì¦ì˜¤':'ë¶„ë…¸',
    'ìê´´ê°':'ê´´ë¡œì›€',	'ê´´ë¡­ë‹¤':'ê´´ë¡œì›€',	'ê³ í†µ':'ê´´ë¡œì›€',
    'ì‹ ê²½ë‚ ì¹´ë¡œì›€':'ì˜ˆë¯¼',	'ì§œì¦':'ì˜ˆë¯¼',	'ì‹ ê²½ì§ˆ':'ì˜ˆë¯¼',
    'ë©”ìŠ¥ê±°ë¦¼':'ì†Œí™”ë¶ˆëŸ‰',
    'í­ì‹':'ì²´ì¤‘ì¦ê°€',	'ì‚´ì°Œë‹¤':'ì²´ì¤‘ì¦ê°€',	'ê³¼ì²´ì¤‘':'ì²´ì¤‘ì¦ê°€',	'ê³¼ì‹ì¦':'ì²´ì¤‘ì¦ê°€',
    'ì„­ì„­í•¨': 'ì„œìš´í•¨',
    'ê¸°ë¶„ìš°ìš¸': 'ì¹¨ìš¸',	'ìš¸ì í•¨':'ì¹¨ìš¸', 'ìƒì¾Œí•˜ì§€ì•ŠìŒ':'ì¹¨ìš¸',	'ê¸°ë¶„ì²˜ì§':'ì¹¨ìš¸',
    'í¥ë¯¸ìƒì‹¤':'ì˜ìš•ê°ì†Œ', 'ë¬´ì˜ìš•':'ì˜ìš•ê°ì†Œ',	'ì˜ìš•ì—†ìŒ':'ì˜ìš•ê°ì†Œ',	'í¥ë¯¸ê°ì†Œ':'ì˜ìš•ê°ì†Œ',	'ì¦ê±°ì›€ê°ì†Œ':'ì˜ìš•ê°ì†Œ',	'ë‚˜ë¥¸í•œ':'ì˜ìš•ê°ì†Œ',	'ì„±ìš•ê°ì†Œ':'ì˜ìš•ê°ì†Œ',
    'ì‡ ì•½':'ë¬´ê¸°ë ¥',	'í˜ì—†ìŒ':'ë¬´ê¸°ë ¥',	'íƒˆë ¥ê°':'ë¬´ê¸°ë ¥',	'ì¡¸ë¦¼':'ë¬´ê¸°ë ¥',	'í˜ë¹ ì§':'ë¬´ê¸°ë ¥',	'ë§ì´ì ':'ë¬´ê¸°ë ¥',	'ì°Œë¿Œë‘¥í•¨':'ë¬´ê¸°ë ¥',
    'ì•ˆì ˆë¶€ì ˆëª»í•¨':'ë¶ˆì•ˆ',	'ì¡°ë§ˆì¡°ë§ˆí•¨':'ë¶ˆì•ˆ',	'ê°€ë§Œíˆëª»ìˆìŒ':'ë¶ˆì•ˆ',	'ë–¨ë¦¼':'ë¶ˆì•ˆ',	'ê±±ì •':'ë¶ˆì•ˆ',	'ì •ì„œë¶ˆì•ˆ(ë¶ˆì•ˆê°)':'ë¶ˆì•ˆ',	'ë¶ˆì•ˆì •':'ë¶ˆì•ˆ',	'ê¸´ì¥':'ë¶ˆì•ˆ',	'ì••ë°•ê°':'ë¶ˆì•ˆ',
    'ê·€ì°®ìŒ':'ëŠ˜ì–´ì§',	'ë¬´ê±°ì›€':'ëŠ˜ì–´ì§',	'ëŠë¦¿ëŠë¦¿':'ëŠ˜ì–´ì§',	'ì²˜ì§':'ëŠ˜ì–´ì§',	'ëˆ„ì›€':'ëŠ˜ì–´ì§',	'ë‘”í™”':'ëŠ˜ì–´ì§',	'ëŠê¸‹':'ëŠ˜ì–´ì§',
    'ìˆ¨ì°¸':'ë‹µë‹µ',	'ì•ì´ ê¹œê¹œí•¨':'ë‹µë‹µ',	'ë§‰ë§‰í•¨':'ë‹µë‹µ',
    'ì§‘ì¤‘ë ¥ì €í•˜':'ì‚¬ê³ ë ¥',	'ì±…ì´ì•ˆì½í˜':'ì‚¬ê³ ë ¥',	'tvì§‘ì¤‘ì•ˆë¨':'ì‚¬ê³ ë ¥',
    'ì¢Œì ˆê°':'ì¢Œì ˆ'
})

import gradio as gr
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration, T5Tokenizer, MarianMTModel, MarianTokenizer, BartTokenizer, BartForConditionalGeneration
from sentence_transformers import SentenceTransformer, util
import pandas as pd
from kiwipiepy import Kiwi
import re

# ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì´í›„ì—ëŠ” ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
print("Loading models...")

# ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base-v2022")
model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base-v2022", num_labels=7)
sentiment_analyzer = pipeline("text-classification", model=model, tokenizer=tokenizer)

# SBERT ëª¨ë¸ ë¡œë“œ
model2 = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
embeddings = model2.encode(texts, convert_to_tensor=True)

# ë²ˆì—­ ë° ì´ëª¨ì§€ ë³€í™˜ ëª¨ë¸ ë¡œë“œ
ko_to_en_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en')
ko_to_en_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')
emoji_tokenizer = BartTokenizer.from_pretrained("KomeijiForce/bart-large-emojilm")
emoji_generator = BartForConditionalGeneration.from_pretrained("KomeijiForce/bart-large-emojilm")

# ë§ì¶¤ë²• êµì • ëª¨ë¸ ë¡œë“œ
corrector_model = T5ForConditionalGeneration.from_pretrained("j5ng/et5-typos-corrector")
corrector_tokenizer = T5Tokenizer.from_pretrained("j5ng/et5-typos-corrector")

kiwi = Kiwi(num_workers=0, model_path=None, load_default_dict=True, integrate_allomorph=True, model_type='knlm', typos=None, typo_cost_threshold=2.5)
print("Models loaded.")

## ì „ì²˜ë¦¬

### ë¬¸ì¥ ë¶„ë¦¬

def split(a):
  sentences = kiwi.split_into_sents(a)
  textArray = [sentence.text for sentence in sentences]
  return textArray

### ì‹ ì¡°ì–´ ì¹˜í™˜

def correct_new_words_before_tokenizing(sentence):
    # ì‹ ì¡°ì–´ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì¹˜í™˜
    for new_word, standard_word in new_words_dict.items():
        if new_word in sentence:
            sentence = sentence.replace(new_word, standard_word)
    return sentence

### ë§ì¶¤ë²• êµì •

# ë””ë°”ì´ìŠ¤ ì„¤ì • (T4 GPU ì‚¬ìš©)
device = "cpu"

# ëª¨ë¸ ë¡œë“œ ì‹œ GPUë¡œ ì´ë™
corrector_model = corrector_model.to(device)
model = model.to(device)
model2 = model2.to(device)
ko_to_en_model = ko_to_en_model.to(device)
emoji_generator = emoji_generator.to(device)

# ê° ë¬¸ì¥ì— ëŒ€í•´ ë§ì¶¤ë²• êµì • ìˆ˜í–‰
def correct_sentence(sentence):
    # ì…ë ¥ ë¬¸ì¥ ì¸ì½”ë”©
    input_encoding = corrector_tokenizer(sentence, return_tensors="pt").to(device)
    input_ids = input_encoding.input_ids
    attention_mask = input_encoding.attention_mask

    # T5 ëª¨ë¸ ì¶œë ¥ ìƒì„±
    output_encoding = corrector_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=128,
        num_beams=5,
        early_stopping=True,
    )

    # ì¶œë ¥ ë¬¸ì¥ ë””ì½”ë”©
    output_text = corrector_tokenizer.decode(output_encoding[0], skip_special_tokens=True)

    # êµì •ëœ ë¬¸ì¥ ì¶”ê°€
    return output_text

### ë²ˆì—­

# í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ í•¨ìˆ˜
def translate_ko_to_en(text):
    tokens = ko_to_en_tokenizer(text, return_tensors="pt", padding=True).to(device)
    translation = ko_to_en_model.generate(**tokens)
    return ko_to_en_tokenizer.decode(translation[0], skip_special_tokens=True)

## Text2Emoji

# ê°ì • ë¶„ì„ ë° ì´ëª¨ì§€ ë³€í™˜ í•¨ìˆ˜
def emoji_convert(text):
    inputs = emoji_tokenizer(text, return_tensors="pt").to(device)
    generated_ids = emoji_generator.generate(inputs["input_ids"], num_beams=4, do_sample=True, max_length=100)
    emoji_text = emoji_tokenizer.decode(generated_ids[0], skip_special_tokens=True).replace(" ", "")
    return emoji_text

### ê°ì •ë¶„ì„

from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬ (80% í•™ìŠµ, 20% ê²€ì¦)
train_texts, eval_texts, train_labels, eval_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)

# í•™ìŠµ ë°ì´í„° í† í¬ë‚˜ì´ì§•
train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors="pt")
eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors="pt")

class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_encodings, train_labels)
eval_dataset = SentimentDataset(eval_encodings, eval_labels)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="steps",
    save_total_limit=2,
    load_best_model_at_end=True,
    save_steps=500,
    eval_steps=500,
    report_to=["none"]
)

# Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

# í•™ìŠµ ì‹œì‘
trainer.train()

from transformers import pipeline

def analyze_emotion(text):
    sentiment_analyzer = pipeline("text-classification", model=model, tokenizer=tokenizer)

    result = sentiment_analyzer(text)[0]

    label_mapping = {
        'LABEL_0': 'í–‰ë³µ',
        'LABEL_1': 'ì¤‘ë¦½',
        'LABEL_2': 'ìŠ¬í””',
        'LABEL_3': 'ê³µí¬',
        'LABEL_4': 'í˜ì˜¤',
        'LABEL_5': 'ë¶„ë…¸',
        'LABEL_6': 'ë†€ëŒ'
    }

    label = result['label']
    emotion = label_mapping.get(label, "ì•Œ ìˆ˜ ì—†ìŒ")  # ë ˆì´ë¸”ì„ ê°ì • ì´ë¦„ìœ¼ë¡œ ë³€í™˜

    if emotion in ['ìŠ¬í””', 'ê³µí¬', 'í˜ì˜¤', 'ë¶„ë…¸', 'ì¤‘ë¦½']:
        return True
    else:
        return False

### ë ˆë“œí”Œë˜ê·¸

# í‚¤ì›Œë“œë³„ í‰ê·  ë²¡í„° ê³„ì‚° (ì¤‘ë¦½ê³¼ í–‰ë³µ ì œì™¸)
keywords = [keyword for keyword in df_merged['keyword'].unique() if keyword not in ['í–‰ë³µ', 'ì¤‘ë¦½']]
keyword_avg_embeddings = {}
for keyword in keywords:
    keyword_texts = df_merged[df_merged['keyword'] == keyword]['text'].tolist()
    if len(keyword_texts) > 0:
        keyword_embeddings = model2.encode(keyword_texts, convert_to_tensor=True)
        keyword_avg_embeddings[keyword] = torch.mean(keyword_embeddings, dim=0, keepdim=True)

import random
# ì¤‘ë¦½ í…ìŠ¤íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ 50ê°œ ìƒ˜í”Œë§
neutral_texts = df_merged[df_merged['keyword'] == 'ì¤‘ë¦½']['text'].tolist()
neutral_sample_texts = random.sample(neutral_texts, min(40, len(neutral_texts)))
neutral_embeddings = model2.encode(neutral_sample_texts, convert_to_tensor=True)

# í–‰ë³µ í…ìŠ¤íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ 50ê°œ ìƒ˜í”Œë§
happy_texts = df_merged[df_merged['keyword'] == 'í–‰ë³µ']['text'].tolist()
happy_sample_texts = random.sample(happy_texts, min(40, len(happy_texts)))
happy_embeddings = model2.encode(happy_sample_texts, convert_to_tensor=True)

# ì¤‘ë¦½ê³¼ í–‰ë³µ ë²¡í„° ì¶”ê°€
keyword_avg_embeddings['ì¤‘ë¦½'] = neutral_embeddings
keyword_avg_embeddings['í–‰ë³µ'] = happy_embeddings

# ê°€ì¥ ìœ ì‚¬í•œ í‚¤ì›Œë“œ ë°˜í™˜ í•¨ìˆ˜ ì •ì˜
def get_most_similar_keyword(target_sentence, model2, keyword_avg_embeddings):
    target_embedding = model2.encode(target_sentence, convert_to_tensor=True).to(device)
    keyword_similarities = {}

    for keyword, avg_embedding in keyword_avg_embeddings.items():
        if avg_embedding.dim() > 1:  # ì¤‘ë¦½ê³¼ í–‰ë³µ ë²¡í„°ëŠ” ì—¬ëŸ¬ ê°œì„, í‰ê·  ë²¡í„° ê³„ì‚°
            avg_embedding = torch.mean(avg_embedding, dim=0).to(device)
        else:
            avg_embedding = avg_embedding.to(device)
        similarity = util.pytorch_cos_sim(target_embedding, avg_embedding).item()
        keyword_similarities[keyword] = similarity

# ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ í‚¤ì›Œë“œ ë°˜í™˜
    most_similar_keyword = max(keyword_similarities, key=keyword_similarities.get)
    return most_similar_keyword, keyword_similarities[most_similar_keyword]

def emotionize(a):
  if a in ["ì£½ê³ ì‹¶ìŒ", "ì£½ì´ê³ ì‹¶ìŒ", "í¬ë§ì—†ìŒ", "ì•ì´ ê¹œê¹œí•¨", "ê°€ì¹˜ ì—†ìŒ", "ë‚­ë– ëŸ¬ì§€ ëì— ìˆëŠ” ëŠë‚Œ"]:
    return 1000
  elif a in [
    "ìì¡´ê°ê°ì†Œ", "íŒ¨ë°°ê°", "í•˜ì°®ìŒ", "ê´´ë¡œì›€", "ëª»ë‚¨", "ì ˆë§ê°", "í¬ê¸°",
    "ì¢Œì ˆê°", "ì ˆë°•",
    "í”¼í•´ì˜ì‹", "ë¹„ì›ƒìŒ", "ëˆˆì´ˆë¦¬", "ëƒ‰ì†Œ", "ê°ì‹œ", "ê¸°ë¶„ìš°ìš¸", "ìŠ¬í””", "ëˆˆë¬¼", "ìš¸ìŒ", "ì£„ì±…ê°", "ìê´´ê°", "ë¹„ì°¸", "ì„œëŸ¬ì›€",
    "ê³ ë…ê°", "ë§‰ë§‰í•¨", "ê³ í†µ"
]:
    return 2
  elif a in [
    "ìš°ìš¸", "ì¹¨ìš¸", "ë©í•¨", "ì„œìš´í•¨", "ì„­ì„­í•¨", "ë¶ˆë§Œ", "ìŠ¬ëŸ¼í”„", "ë‚˜ë¥¸í•¨", "í˜ì—†ìŒ", "ì²˜ì§", "í˜ë¹ ì§", "ì°Œë¿Œë‘¥í•¨", "ë§ì´ì ", "ëŠë¦¿ëŠë¦¿í•¨",
    "ë¬´ì˜ìš•", "ìˆ˜ë©´ì¥ì• ", "ì§‘ì¤‘ë ¥ê°ì†Œ", "ì˜ìš•ì—†ìŒ", "ì ì´ì•ˆì˜´", "ë¬´ê¸°ë ¥", "ì˜ìš•ê°ì†Œ", "ë¶ˆë©´", "í¥ë¯¸ìƒì‹¤", "ì‹ìš•ë¶€ì§„",
    "ê³µí—ˆ", "ë‚™ ì—†ìŒ", "ì¦ê±°ì›€ê°ì†Œ", "ë‹µë‹µí•¨", "ê¸°ë¶„ì²˜ì§", "ì§œì¦", "ì•ˆì ˆë¶€ì ˆëª»í•¨", "ì¡°ë§ˆì¡°ë§ˆí•¨", "í˜¼ë€", "í—ˆë¬´í•¨", "í—ˆíƒˆ",
    "ì†Œì‹¬", "ìì‹ ê°ê°ì†Œ", "ìœ„ì¶•", "ìš°ìœ ë¶€ë‹¨", "ê²°ë‹¨ë ¥ì—†ìŒ",
    "ì†ìƒí•¨", "í›„íšŒ", "í›„íšŒí•¨", "ì–µìš¸í•¨",
    "ì™¸ë¡œì›€", "ì“¸ì“¸í•¨"
]:
    return 1
  else:
    return 0

# riskì˜ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì‹œì§€ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
import random

comforting_messages = [
    "ì˜¤ëŠ˜ í˜ë“¤ì—ˆë‹¤ë©´ ì ì‹œ ì‰¬ì–´ë„ ê´œì°®ì•„ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•œ ì‚¬ëŒì´ì—ìš”. ğŸŒŸ",
    "ì§€ê¸ˆì€ ì–´ë µê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ë¶„ëª…íˆ ë” ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”. ë‹¹ì‹ ì˜ ë§ˆìŒì„ ë¯¿ì–´ìš”. ğŸ’–",
    "í˜ë“¤ ë• í˜¼ì ì§Šì–´ì§€ì§€ ë§ê³  ì£¼ë³€ì— ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”. ë‹¹ì‹ ì€ í˜¼ìê°€ ì•„ë‹ˆì—ìš”. ğŸ¤—",
    "ìŠ¤ìŠ¤ë¡œë¥¼ ë‹¤ë…ì—¬ ì£¼ì„¸ìš”. ë‹¹ì‹ ì€ ì§€ê¸ˆë„ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆì–´ìš”. ğŸŒˆ",
    "ì‘ì€ í•œ ê±¸ìŒë„ ì†Œì¤‘í•´ìš”. ì²œì²œíˆ, ê¾¸ì¤€íˆ ê±¸ì–´ê°€ìš”. ğŸŒ±",
    "ì˜¤ëŠ˜ë„ ìµœì„ ì„ ë‹¤í•œ ë‹¹ì‹ ì—ê²Œ ë”°ëœ»í•œ ì‘ì›ì„ ë³´ëƒ…ë‹ˆë‹¤. â˜€ï¸",
    "ë•Œë¡œëŠ” ì‰¬ì–´ê°€ëŠ” ê²ƒë„ í•„ìš”í•œ ì‹œê°„ì´ëë‹ˆë‹¤. ê´œì°®ì•„ìš”. ğŸŒ¼",
    "ì‘ì€ ê¸°ì¨ í•˜ë‚˜ì”© ì°¾ì•„ë³´ë©° ìŠ¤ìŠ¤ë¡œì—ê²Œ ì›ƒìŒì„ ì„ ë¬¼í•´ ì£¼ì„¸ìš”. ğŸ˜Š",
    "ì–´ë‘ìš´ ê¸¸ë„ ì–¸ì  ê°€ í™˜íˆ ë¹„ì¶°ì§ˆ ê±°ì˜ˆìš”. ì¡°ê¸ˆë§Œ ë” í˜ë‚´ë´ìš”. ğŸŒ™",
    "ë‹¹ì‹ ì˜ í•˜ë£¨ëŠ” ëˆ„êµ°ê°€ì—ê²Œ í° ìœ„ì•ˆì´ ë˜ê³  ìˆì„ ê±°ì˜ˆìš”. ğŸ’Œ",
    "ì§€ê¸ˆë„ ë‹¹ì‹ ì€ ì¶©ë¶„íˆ íŠ¹ë³„í•˜ê³  ì†Œì¤‘í•œ ì¡´ì¬ì˜ˆìš”. ğŸ’",
    "ë§¤ì¼ ì‘ì€ ë³€í™”ê°€ ëª¨ì—¬ í° ì„±ì¥ì„ ì´ë£¬ë‹µë‹ˆë‹¤. ìì‹ ì„ ë¯¿ì–´ì£¼ì„¸ìš”. ğŸŒ»",
    "ë‹¹ì‹ ì€ ì¡´ì¬ë§Œìœ¼ë¡œë„ ë§ì€ ì‚¬ëŒì—ê²Œ ê¸°ì¨ì´ ë˜ê³  ìˆì–´ìš”. â¤ï¸",
    "ë§ˆìŒì˜ ì§ì´ ë¬´ê±°ìš¸ ë• ê¹Šê²Œ ìˆ¨ì„ ì‰¬ì–´ë³´ì„¸ìš”. ê´œì°®ì•„ì§ˆ ê±°ì˜ˆìš”. ğŸƒ",
    "ì˜¤ëŠ˜ ë‹¹ì‹ ì´ í•´ë‚¸ ëª¨ë“  ì¼ë“¤, ë‹¤ ë©‹ì§„ ê±¸ìŒì´ì—ìš”. ìë‘ìŠ¤ëŸ¬ì›Œìš”. ğŸï¸"
]
cautionary_messages = [
    "í˜¹ì‹œ ë„ˆë¬´ ë§ì€ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ê³  ìˆëŠ” ê±´ ì•„ë‹Œê°€ìš”? ì ì‹œ ë©ˆì¶”ê³  ìì‹ ì„ ëŒì•„ë³´ì„¸ìš”. âš ï¸",
    "ìµœê·¼ ê°ì •ì´ ë¬´ê²ë‹¤ë©´ ë¯¿ì„ ìˆ˜ ìˆëŠ” ì‚¬ëŒê³¼ ëŒ€í™”í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ğŸ›‘",
    "í˜ë“  ìƒí™©ì—ì„œ ë¬´ë¦¬í•˜ë ¤ í•˜ì§€ ë§ê³  ìŠ¤ìŠ¤ë¡œë¥¼ ë³´í˜¸í•˜ëŠ” ì„ íƒì„ í•´ë³´ì„¸ìš”. ğŸ•Šï¸",
    "ì´ ê°ì •ì´ ì˜¤ë˜ ì§€ì†ëœë‹¤ë©´ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ì—ìš”. ğŸ©º",
    "ì–´ë–¤ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „, ê°ì •ì— íœ©ì“¸ë¦¬ì§€ ì•Šê³  ì‹ ì¤‘í•˜ê²Œ ìƒê°í•´ë³´ì„¸ìš”. ğŸ›¡ï¸",
    "í˜¹ì‹œ ìŠ¤ìŠ¤ë¡œë¥¼ ë„ˆë¬´ ëª°ì•„ë¶™ì´ê³  ìˆì§„ ì•Šì€ê°€ìš”? ìì‹ ì—ê²Œ íœ´ì‹ì„ í—ˆë½í•´ ì£¼ì„¸ìš”. ğŸš¦",
    "ê°ì •ì´ ê²©í•´ì§ˆ ë• ì ì‹œ ë©ˆì¶”ê³  ìì‹ ì„ ëŒì•„ë³´ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ì„¸ìš”. â³",
    "ë§ˆìŒì´ ì§€ì³¤ë‹¤ë©´ ë„ì›€ì„ ìš”ì²­í•´ë„ ê´œì°®ì•„ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•˜ë‹ˆê¹Œìš”. ğŸ«‚",
    "ìŠ¤ìŠ¤ë¡œ ê°ë‹¹í•˜ê¸° í˜ë“  ìƒí™©ì´ë¼ë©´ í˜¼ì ë™ë™ ì•“ì§€ ë§ê³  ì£¼ë³€ì— ì•Œë¦¬ì„¸ìš”. ğŸ“£",
    "í˜ë“  ìˆœê°„ì—ëŠ” ëª¨ë“  ê±¸ ë‚´ë ¤ë†“ê³  ë§ˆìŒì˜ í‰í™”ë¥¼ ì°¾ëŠ” ê²ƒì´ í•„ìš”í•  ìˆ˜ë„ ìˆì–´ìš”. ğŸ•Šï¸",
    "ê°€ë”ì€ ìì‹ ì˜ ê°ì •ì„ ê¸°ë¡í•´ë³´ì„¸ìš”. ì´ë¥¼ í†µí•´ ë§ˆìŒì„ ì •ë¦¬í•  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”. âœï¸",
    "ìµœê·¼ í˜ë“  ì¼ì´ ë§ì•˜ë‹¤ë©´ ì¶©ë¶„íˆ ì‰¬ê³  ìŠ¤ìŠ¤ë¡œë¥¼ ìœ„ë¡œí•˜ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ì„¸ìš”. ğŸŒŒ",
    "ë„ˆë¬´ ê¹Šì€ ê³ ë¯¼ì— ë¹ ì§€ê¸°ë³´ë‹¨ ë¯¿ì„ ìˆ˜ ìˆëŠ” ì‚¬ëŒê³¼ í•¨ê»˜ í•´ê²°í•´ë³´ì„¸ìš”. ğŸ¤",
    "ë‹¹ì‹ ì˜ ê°ì •ì€ ì†Œì¤‘í•©ë‹ˆë‹¤. ì´ë¥¼ ë¬´ì‹œí•˜ê±°ë‚˜ ì–µëˆ„ë¥´ì§€ ì•Šì•„ë„ ë¼ìš”. ğŸŒ·",
    "ìŠ¤ìŠ¤ë¡œë¥¼ ëŒë³´ëŠ” ê²ƒì´ì•¼ë§ë¡œ ì§„ì •í•œ ìš©ê¸°ì…ë‹ˆë‹¤. ë‹¹ì‹ ì„ ì‘ì›í•´ìš”. ğŸ›¡ï¸"
]



def showMessage(risk):
  risk=float(risk)
  if (2/3)> risk >= (1/3):
    return(comforting_messages[random.randint(0, len(comforting_messages) - 1)])
  elif risk >= (2/3):
    return(cautionary_messages[random.randint(0, len(cautionary_messages) - 1)]+"\n"+"\n" + "ì§€ê¸ˆ í˜ë“  ìƒí™©ì— ì²˜í•´ ìˆê±°ë‚˜ ê°ì •ì ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ê³„ì‹ ë‹¤ë©´,"+"\n"+"ì „ë¬¸ì ì¸ ë„ì›€ì„ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤." + "\n" + "í•œêµ­ ì •ì‹ ê±´ê°• ìƒë‹´ì „í™” 1577-0199")
  else :
    return ""

## ìµœì¢…

# ì „ì²´ í•¨ìˆ˜ ì‹¤í–‰
def correct(a):
  korean_text = a
  slang_korean= correct_new_words_before_tokenizing(korean_text)
  correct_korean=correct_sentence(slang_korean)
  return correct_korean

def text2emoji(a):
  translated_text = translate_ko_to_en(a)
  emoji_text = emoji_convert(translated_text)
  return emoji_text
def emotion_texting(a):
  most_similar_keyword, similarity = get_most_similar_keyword(a, model2, keyword_avg_embeddings)
  return most_similar_keyword

def final(user_input):
    count = 0
    emotion = 0
    analyze= 0
    diary = ""
    emoji = ""

    textArray = split(user_input)

    for i in textArray:
        count += 1
        correct_korean = correct(i)
        emoji = text2emoji(correct_korean)
        analyze=analyze_emotion(i)
        redscore = emotion_texting(i)
        diary = diary + i + emoji + "\t"
        if analyze == True:
          emotion += emotionize(redscore)
        else:
          emotion+=0
    output_message = showMessage(emotion / count)
    output = diary + '\n'+'\n'+'\n'+ output_message
    return output

# Gradio ì¸í„°í˜ì´ìŠ¤ ì •ì˜
def run_final(user_input):
    return final(user_input)

css = """
/* ì „ì²´ ì»¨í…Œì´ë„ˆ ìˆ˜ì§ ì •ë ¬ */
.gradio-container {
    display: flex;
    justify-content: center;
    align-items: center;
    width: 900px;
    height: 100vh; /* ì „ì²´ í™”ë©´ì— ìˆ˜ì§ ì •ë ¬ */
    padding-top: 200px;

    gap: 20px; /* ìš”ì†Œ ê°„ ê°„ê²© */
    background-color: #FFF8dc;
}

/* ì…ë ¥ê³¼ ì¶œë ¥ ë°•ìŠ¤ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜ */
.gradio-input-output {
    width: 300px;
    gap: 20px; /* ìš”ì†Œ ê°„ ê°„ê²© */
}

.gap.svelte-vt1mxs {
  width: 900px;
  padding-top: 12vh;
}

.gradio-container-5-6-0 .prose * {
  width: 900px;
  text-align: center;
}



/* description ìŠ¤íƒ€ì¼ */
#interface-description {
    width: 900px;
    text-align: center;
    margin-bottom: 20px;
    font-size: 2rem;
}

.gradio-container.gradio-container-5-6-0 .contain button {
  height: 60px;
  font-size: 1.5rem;
}

/* input & output container */

div.svelte-633qhp .block {
  background-color: #fffaf0;
}

/* input label */
span.svelte-1gfkn6j {
  font-size: 1.2rem;
  font-weight: 600;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
button {
    padding: 10px 20px;
    background-color: #ff7f50;
    color: white;
    border: none;
    border-radius: 5px;
    cursor: pointer;
}

button:hover {
    background-color: #ff6347;
}

/* Emotify ğŸ¤© */
h1 {
  font-size : 3rem;
}

/*  í…ìŠ¤íŠ¸ì— ê°ì •ì„ ë”í•˜ë‹¤ğŸ˜‰ */
h3 {
  font-size : 2rem;
}
"""

title = "Emotify ğŸ¤©"  # í˜ì´ì§€ ì œëª© ì„¤ì •

# Gradio Blocksë¥¼ ì‚¬ìš©í•˜ì—¬ UI êµ¬ì„±
with gr.Blocks(css=css, title=title) as demo:
    gr.Markdown("# EmotifyğŸ¤© \n ### í…ìŠ¤íŠ¸ì— ê°ì •ì„ ë”í•˜ë‹¤ğŸ˜‰")

    # ì…ë ¥ê³¼ ì¶œë ¥ ë°•ìŠ¤ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜
    with gr.Row(elem_id="gradio-input-output"):
        input_box = gr.Textbox(lines=20, placeholder="ê°ì •ì„ í‘œí˜„í•´ ë³´ì„¸ìš”.", label="ê°ì • í‘œí˜„í•˜ê¸°")
        output_box = gr.Textbox(lines=20, label="ê°ì • ë¶ˆì–´ë„£ê¸°")

    # ë²„íŠ¼ ì¶”ê°€
    submit_button = gr.Button("ê°ì • ë¶ˆì–´ë„£ê¸°ğŸ˜™")  # ë²„íŠ¼ í…ìŠ¤íŠ¸ ì„¤ì •

    # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°
    submit_button.click(fn=final, inputs=input_box, outputs=output_box)

# Gradio ì‹¤í–‰
demo.launch(share=True)
